{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import gc\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from functools import partial\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations\n",
    "class CFG:\n",
    "    resnext = 'resnext50_32x4d'\n",
    "    deit = 'deit_base_patch16_384'\n",
    "    efficientnet = 'tf_efficientnet_b4_ns' \n",
    "    n_class = 5\n",
    "    trn_fold = [1,2,3,4,5]\n",
    "    data_dir = './data/test_images'\n",
    "    result_dir = './result'\n",
    "    batch_size = 8\n",
    "    deit = 'deit_base_patch16_384'\n",
    "    img_size = (512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (512,512)\n",
    "submission = pd.DataFrame(columns={\"image_id\",\"label\"})\n",
    "submission[\"image_id\"] = os.listdir(CFG.data_dir)\n",
    "submission[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlykeras = False\n",
    "        \n",
    "used_models_pytorch = {\"efficientnet\": [f'./checkpoint/Efficientnet/f{fold}.pth' for fold in CFG.trn_fold],\n",
    "                       \"resnext\": [f'./checkpoint/Resnext50/f{fold}.pth' for fold in CFG.trn_fold],\n",
    "                       \"deit\": [f'./checkpoint/Deit/f{fold}.pth' for fold in CFG.trn_fold]\n",
    "                      }\n",
    "\n",
    "used_models_keras = {\"cropnet\": \"./checkpoint/Cropnet\"}\n",
    "\n",
    "# We used this flag for testing different ensembling approaches\n",
    "stacked_mean = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util\n",
    "def load(model_path, net):\n",
    "    dict_model = torch.load(model_path)\n",
    "    net.load_state_dict(dict_model)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNext50_32x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 클래스는 resnext와 deit 공통으로 사용할 수 있다.\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.lst_input = os.listdir(self.data_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst_input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = cv2.imread(os.path.join(self.data_dir, self.lst_input[index]), cv2.IMREAD_COLOR)\n",
    "        input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)  # result of input shape is y,x,c\n",
    "\n",
    "        if self.transform:\n",
    "            input = self.transform(image=input)['image']\n",
    "\n",
    "        data = {'input' : input}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnext(nn.Module):\n",
    "    def __init__(self, network=CFG.resnext, n_class=CFG.n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(network, pretrained=pretrained)\n",
    "        n_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(n_features, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"resnext\" in used_models_pytorch:\n",
    "    def transform_test():\n",
    "        return A.Compose([A.Resize(512, 512),\n",
    "                        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                        ToTensorV2()])\n",
    "\n",
    "    def inference(models, loader_test, device):\n",
    "        pred = [] \n",
    "        for batch, data in enumerate(loader_test, 1):\n",
    "            input = data['input'].to(device)\n",
    "            avg_output = []\n",
    "            \n",
    "            for model in models:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    output = model(input)\n",
    "                avg_output.append(output.softmax(1).to('cpu').numpy())\n",
    "            avg_output = np.mean(avg_output, axis=0)\n",
    "            pred.append(avg_output)\n",
    "        return np.concatenate(pred) \n",
    "    \n",
    "    \n",
    "    # inference\n",
    "    net = resnext(CFG.resnext, CFG.n_class, pretrained=False).to(device)  \n",
    "    models = [load(f, net) for f in used_models_pytorch[\"resnext\"]]\n",
    "    dataset_test = Dataset(data_dir=CFG.data_dir, transform=transform_test())\n",
    "    loader_test = DataLoader(dataset_test, batch_size=CFG.batch_size, shuffle=False, num_workers=8, pin_memory=True) \n",
    "    \n",
    "    pred = inference(models, loader_test, device)\n",
    "\n",
    "    # submission save\n",
    "    predictions_resnext = pd.DataFrame(columns={\"image_id\"})\n",
    "    predictions_resnext[\"image_id\"] = submission[\"image_id\"].values\n",
    "    predictions_resnext['image_path_id'] = CFG.data_dir + predictions_resnext['image_id'].astype(str)\n",
    "\n",
    "    predictions_resnext['resnext'] = [np.squeeze(p) for p in pred]\n",
    "    predictions_resnext = predictions_resnext.drop([\"image_path_id\"], axis=1)\n",
    "    \n",
    "\n",
    "    # empty_cache() 를 통해 사용하지 않으면서 캐시된 메모리들을 해제할 수 있다.\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        del(model)\n",
    "        del(states)\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>resnext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>[0.016739627, 0.009845901, 0.21215543, 0.01410...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id                                            resnext\n",
       "0  2216849948.jpg  [0.016739627, 0.009845901, 0.21215543, 0.01410..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_resnext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deit(nn.Module):\n",
    "    def __init__(self, network, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('facebookresearch/deit:main', network, pretrained=pretrained)\n",
    "        self.n_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(self.n_features,n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kerrykim/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "if \"deit\" in used_models_pytorch:    \n",
    "    def transform_test():\n",
    "        return A.Compose([A.CenterCrop(384, 384, p=1.),\n",
    "                A.Resize(384, 384),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "                ToTensorV2(p=1.0)], p=1.)\n",
    "\n",
    "    def inference(models, loader_test, device):\n",
    "        pred = [] \n",
    "        for batch, data in enumerate(loader_test, 1):\n",
    "            input = data['input'].to(device)\n",
    "            avg_output = []            \n",
    "            \n",
    "            for model in models:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(input)\n",
    "                avg_output.append(output.softmax(1).to('cpu').numpy())\n",
    "                \n",
    "            avg_output = np.mean(avg_output, axis=0)\n",
    "            pred.append(avg_output)\n",
    "            \n",
    "        return np.concatenate(pred)    \n",
    "    \n",
    "    # inference\n",
    "    net = deit(CFG.deit, CFG.n_class, pretrained=False).to(device)   \n",
    "    models = [load(f, net) for f in used_models_pytorch[\"deit\"]]\n",
    "\n",
    "    dataset_test = Dataset(data_dir=CFG.data_dir, transform=transform_test())\n",
    "    loader_test = DataLoader(dataset_test, batch_size=CFG.batch_size, shuffle=False, num_workers=8, pin_memory=True) \n",
    "    pred = inference(models, loader_test, device)\n",
    "\n",
    "    \n",
    "    # submission save\n",
    "    predictions_deit = pd.DataFrame(columns={\"image_id\"})\n",
    "    predictions_deit[\"image_id\"] = submission[\"image_id\"].values\n",
    "    predictions_deit['image_path_id'] = CFG.data_dir + predictions_deit['image_id'].astype(str)\n",
    "\n",
    "    predictions_deit['deit'] = [np.squeeze(p) for p in pred]\n",
    "    predictions_deit = predictions_deit.drop([\"image_path_id\"], axis=1)\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        for model in models:\n",
    "            del(model)\n",
    "    except:\n",
    "        pass\n",
    "    models = []\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>deit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>[0.025769398, 0.077960335, 0.43972144, 0.01591...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id                                               deit\n",
       "0  2216849948.jpg  [0.025769398, 0.077960335, 0.43972144, 0.01591..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_deit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class efficientnet(nn.Module):\n",
    "    def __init__(self, network, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(network, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"efficientnet\" in used_models_pytorch:  \n",
    "\n",
    "    transform = A.Compose([A.Resize(512, 512),\n",
    "                            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "                            A.Transpose(p=0.75),\n",
    "                            A.HorizontalFlip(p=0.75),\n",
    "                            A.VerticalFlip(p=0.75),\n",
    "                            A.RandomRotate90(p=0.75)], p=1.)\n",
    "\n",
    "    def read_preprocess_file(image, normalize=False):   #  원래 코드랑 달리 이미지를 직접 받는 걸로 고쳤다.\n",
    "        if normalize:\n",
    "            img_scaled = np.array(image)/ 255.0\n",
    "        else:\n",
    "            img_scaled = np.array(image)\n",
    "        img_scaled = img_scaled.astype(np.float32)\n",
    "\n",
    "        return (image.size[0], image.size[1]), img_scaled  # PIL image는 size로 받고 np array는 shape로 값을 받는다.\n",
    "\n",
    "\n",
    "    def create_image_tiles(origin_dim, processed_img): \n",
    "        crop_size = 512\n",
    "        img_list = []\n",
    "        for x in [0, origin_dim[1] - crop_size]:\n",
    "            for y in [0, origin_dim[0] - crop_size]:\n",
    "                img_list.append(processed_img[x:x+crop_size , y:y+crop_size,:])\n",
    "        img_list.append(cv2.resize(processed_img[:, 100:700 ,:], dsize=(crop_size, crop_size)))\n",
    "        return np.array(img_list)\n",
    "\n",
    "\n",
    "    def augment_tiles_light(tiles, ttas=2): \n",
    "        holdout = np.broadcast_to(tiles[-1,:,:,:],(ttas,) + tiles.shape[1:])       \n",
    "        lst = np.concatenate([tiles[:-1,:,:,:] for _ in range(ttas)], axis=0)\n",
    "\n",
    "        augmented_batch = np.array([transform(image=lst[i, :, :, :])['image'] for i in range(8)])      \n",
    "        return np.concatenate([augmented_batch, holdout], axis=0) \n",
    "    \n",
    "    def inference(models, loader_test, device):\n",
    "        pred = [] \n",
    "        for batch, data in enumerate(loader_test, 1):          \n",
    "            input = np.squeeze(data['input']).to('cpu').detach().numpy() \n",
    "\n",
    "            # Dataset에서 Input을 PIL로 받아서 처리해 줄 수 있는 구문이지만 Dataset을 새로 정의해주지 않기 위해 이렇게 되돌린다\n",
    "            # Augmentation code가 모두 PIL 이미지 기준으로 되어 있다. np.array로 바꿔주려면 한참 걸림 ㅠㅠ\n",
    "            pil_input = Image.fromarray(input)\n",
    "            input = augment_tiles_light(create_image_tiles(*read_preprocess_file(pil_input))) # result.shape (10, 512, 512, 3)\n",
    "       \n",
    "            avg_output = []\n",
    "            \n",
    "#           input의 차원은 (10, 3, 512, 512)\n",
    "#             for image in input:\n",
    "#                 print(image.shape)\n",
    "#                 input = np.moveaxis(image[:, :, :], -1, 0) # result.shape (512, 512, 3)\n",
    "#                 input = np.expand_dims(input,0)\n",
    "#                 input = torch.from_numpy(input)\n",
    "#                 input = input.to(device)\n",
    "\n",
    "#             for i in range(10):\n",
    "#                 print(input[i, :, :, :])\n",
    "\n",
    "\n",
    "            # 글로벌 변수로 input을 사용하고 있기 때문에 로컬 변수를 사용할 때는 변수명을 변경하여 사용한다.\n",
    "            for i in range(1):\n",
    "                image = input[9, :, : ,:]\n",
    "                image = np.moveaxis(image[:, :, :], -1, 0) # result.shape (512, 512, 3) // 차원이 축소된다.\n",
    "                image = np.expand_dims(image,0)\n",
    "                image = torch.from_numpy(image)\n",
    "                image = image.to(device)\n",
    "\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    tta_output=[]\n",
    "                    for model in models:\n",
    "                        model.eval()\n",
    "\n",
    "                        output = model(image)\n",
    "                        tta_output.append(output.softmax(1).to('cpu').numpy())\n",
    "                \n",
    "                tta_output = np.mean(tta_output, axis=0)\n",
    "                avg_output.append(tta_output)\n",
    "                \n",
    "            avg_output = np.mean(avg_output, axis=0)\n",
    "            pred.append(avg_output)\n",
    "            \n",
    "        return np.concatenate(pred)    \n",
    "    \n",
    "    # inference\n",
    "    net = efficientnet(CFG.efficientnet, CFG.n_class, pretrained=False).to(device)   \n",
    "    models = [load(f, net) for f in used_models_pytorch[\"efficientnet\"]]\n",
    "\n",
    "    dataset_test = Dataset(data_dir=CFG.data_dir, transform=None)    # 직접 해줘야 한다.\n",
    "    loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=8, pin_memory=True) \n",
    "    \n",
    "    # 아직 argmax를 하지 않았기 때문에 probability인 상태이다.\n",
    "    pred = inference(models, loader_test, device)\n",
    "\n",
    "    \n",
    "    # submission save\n",
    "    predictions_eff = pd.DataFrame(columns={\"image_id\"})\n",
    "    predictions_eff[\"image_id\"] = submission[\"image_id\"].values\n",
    "    predictions_eff['image_path_id'] = CFG.data_dir + predictions_eff['image_id'].astype(str)\n",
    "\n",
    "    predictions_eff['efficientnet'] = [np.squeeze(p) for p in pred]\n",
    "    predictions_eff = predictions_eff.drop([\"image_path_id\"], axis=1)\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        for model in models:\n",
    "            del(model)\n",
    "    except:\n",
    "        pass\n",
    "    models = []\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>efficientnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id               efficientnet\n",
       "0  2216849948.jpg  [0.0, 0.0, 0.0, 1.0, 0.0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_file(img_path, normalize=False):\n",
    "    image = Image.open(img_path)\n",
    "    if normalize:\n",
    "        img_scaled = np.array(image)/ 255.0\n",
    "    else:\n",
    "        img_scaled = np.array(image)\n",
    "    img_scaled = img_scaled.astype(np.float32)\n",
    "    print(image)\n",
    "    return (image.size[0], image.size[1]), img_scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_crop_image(processed_img):\n",
    "    image = tf.image.central_crop(processed_img, 0.8)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    return np.expand_dims(image, 0)    # 만약 np.expand_dims하지 않으면 TensorShape([224, 224, 3])의 shape가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenet3(img_size=(224,224), weights=\"./checkpoint/Cropnet\"):\n",
    "    classifier = hub.KerasLayer(weights)\n",
    "    model = tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=img_size + (3,)), # input shape= 224,224,3 \n",
    "                                 hub.KerasLayer(classifier, trainable=False)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_unknown(propabilities):\n",
    "    return propabilities[:,:-1] + np.expand_dims(propabilities[:,-1]/5, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_predict_tfhublayer(img_path, modelinstance):\n",
    "    # 여기서 하단의 True는 normalize에 대한 인자로 normalize 하겠다는 의미\n",
    "    img = cut_crop_image(read_preprocess_file(img_path, True)[1])\n",
    "    yhat = modelinstance.predict(img)\n",
    "    return np.mean(distribute_unknown(yhat), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_vote(image_list, modelinstances, onlykeras):\n",
    "    predictions = [] \n",
    "    with tqdm(total=len(image_list)) as process_bar:       \n",
    "        for img_path in image_list:\n",
    "            process_bar.update(1)  \n",
    "            Yhats = np.vstack([func(img_path, modelinstance) for func, modelinstance in modelinstances])\n",
    "            print(Yhats)\n",
    "        \n",
    "        # onlykeras = False 해당구문 실행 안됨. 아마 결과값(predictions)은 확률값으로 나올듯 하긴 그래야 파이토치로 구현한 레즈넷등이랑 앙상블함\n",
    "        if onlykeras:\n",
    "            predictions.append(np.argmax(np.sum(Yhats, axis=0)))\n",
    "        else:\n",
    "            predictions.append(Yhats)    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inference_models = []\n",
    "\n",
    "if \"cropnet\" in used_models_keras:   \n",
    "    model_mobilenet = build_mobilenet3(weights=used_models_keras[\"cropnet\"])    # 크롭넷 모델 정의\n",
    "    inference_models.append((multi_predict_tfhublayer, model_mobilenet)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x600 at 0x7FAC4C7649D0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00399159 0.00370761 0.8709404  0.00654031 0.11482014]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submission[\"label\"] = predict_and_vote([os.path.join(CFG.data_dir, id) for id in submission[\"image_id\"].values], inference_models, onlykeras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0039915913, 0.0037076084, 0.8709404, 0.006...</td>\n",
       "      <td>2216849948.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label        image_id\n",
       "0  [[0.0039915913, 0.0037076084, 0.8709404, 0.006...  2216849948.jpg"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "try:\n",
    "    del inference_models[:]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list(used_models_keras.keys())) <= 1:    \n",
    "    submission.loc[:,list(used_models_keras)[0]] = submission[\"label\"].explode()\n",
    "else:\n",
    "    tmp = (submission['label'].transform([lambda x:x[0], lambda x:x[1]]).set_axis(list(used_models_keras.keys()), axis=1, inplace=False))\n",
    "    submission = submission.merge(tmp, right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>image_id</th>\n",
       "      <th>cropnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0039915913, 0.0037076084, 0.8709404, 0.006...</td>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>[0.0039915913, 0.0037076084, 0.8709404, 0.0065...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label        image_id  \\\n",
       "0  [[0.0039915913, 0.0037076084, 0.8709404, 0.006...  2216849948.jpg   \n",
       "\n",
       "                                             cropnet  \n",
       "0  [0.0039915913, 0.0037076084, 0.8709404, 0.0065...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"label\"] = 0\n",
    "\n",
    "if \"resnext\" in used_models_pytorch:\n",
    "    submission = submission.merge(predictions_resnext, on=\"image_id\")\n",
    "    \n",
    "if \"deit\" in used_models_pytorch:\n",
    "    submission = submission.merge(predictions_deit, on=\"image_id\")\n",
    "    \n",
    "if \"efficientnet\" in used_models_pytorch:\n",
    "    submission = submission.merge(predictions_eff, on=\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>image_id</th>\n",
       "      <th>cropnet</th>\n",
       "      <th>resnext</th>\n",
       "      <th>deit</th>\n",
       "      <th>efficientnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>[0.0039915913, 0.0037076084, 0.8709404, 0.0065...</td>\n",
       "      <td>[0.016739627, 0.009845901, 0.21215543, 0.01410...</td>\n",
       "      <td>[0.025769398, 0.077960335, 0.43972144, 0.01591...</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.9, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        image_id                                            cropnet  \\\n",
       "0      0  2216849948.jpg  [0.0039915913, 0.0037076084, 0.8709404, 0.0065...   \n",
       "\n",
       "                                             resnext  \\\n",
       "0  [0.016739627, 0.009845901, 0.21215543, 0.01410...   \n",
       "\n",
       "                                                deit  \\\n",
       "0  [0.025769398, 0.077960335, 0.43972144, 0.01591...   \n",
       "\n",
       "                efficientnet  \n",
       "0  [0.0, 0.1, 0.0, 0.9, 0.0]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stacked_mean:\n",
    "    # zip 함수는 값을 받아 새로운 배열을 만든다. vit와 resnext를 받아서 e = (vit, resnext)이란 zip 데이터  타입을 만들어 평균함\n",
    "    submission[\"stage_1\"] = submission.apply(lambda x: [np.mean(e) for e in zip(x[\"deit\"], x[\"resnext\"])], axis=1)\n",
    "    # stage1은 vit와 resnext를 평균취해서 새로운 확률을 만들고\n",
    "    # stage1과 cropnet, efficientnet 값을 모두 더한후 argmax 취해서 가장 큰 값을 라벨로 얻는다.\n",
    "    submission[\"label\"] = submission.apply(lambda x: np.argmax(\n",
    "        [np.sum(e) for e in zip(x[\"cropnet\"],x[\"stage_1\"], x[\"efficientnet\"])]), axis=1)        \n",
    "else:\n",
    "    submission[\"label\"] = submission.apply(lambda row: np.argmax(\n",
    "        [np.sum(e) for e in zip(*[row[m] for m in list(used_models_pytorch.keys())+list(used_models_keras.keys())])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>image_id</th>\n",
       "      <th>cropnet</th>\n",
       "      <th>resnext</th>\n",
       "      <th>deit</th>\n",
       "      <th>efficientnet</th>\n",
       "      <th>stage_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>[0.0039915913, 0.0037076084, 0.8709404, 0.0065...</td>\n",
       "      <td>[0.016739627, 0.009845901, 0.21215543, 0.01410...</td>\n",
       "      <td>[0.025769398, 0.077960335, 0.43972144, 0.01591...</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.9, 0.0]</td>\n",
       "      <td>[0.021254513, 0.04390312, 0.32593843, 0.015009...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        image_id                                            cropnet  \\\n",
       "0      2  2216849948.jpg  [0.0039915913, 0.0037076084, 0.8709404, 0.0065...   \n",
       "\n",
       "                                             resnext  \\\n",
       "0  [0.016739627, 0.009845901, 0.21215543, 0.01410...   \n",
       "\n",
       "                                                deit  \\\n",
       "0  [0.025769398, 0.077960335, 0.43972144, 0.01591...   \n",
       "\n",
       "                efficientnet  \\\n",
       "0  [0.0, 0.1, 0.0, 0.9, 0.0]   \n",
       "\n",
       "                                             stage_1  \n",
       "0  [0.021254513, 0.04390312, 0.32593843, 0.015009...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[[\"image_id\",\"label\"]].to_csv(os.path.join(CFG.result_dir,\"submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id,label\r\n",
      "2216849948.jpg,2\r\n"
     ]
    }
   ],
   "source": [
    "!head ./result/submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[1,2,3],[3,4,5], [5,6,7], [7,8,8]], [[2,2,2],[3,3,3], [5,6,7], [7,8,8]], [[2,2,2],[3,3,3], [5,6,7], [7,8,8]], [[2,2,2],[3,3,3], [5,6,7], [7,8,8]]]\n",
    "b= np.array(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [3 4 5]\n",
      " [5 6 7]\n",
      " [7 8 8]]\n",
      "[[2 2 2]\n",
      " [3 3 3]\n",
      " [5 6 7]\n",
      " [7 8 8]]\n",
      "[[2 2 2]\n",
      " [3 3 3]\n",
      " [5 6 7]\n",
      " [7 8 8]]\n",
      "[[2 2 2]\n",
      " [3 3 3]\n",
      " [5 6 7]\n",
      " [7 8 8]]\n"
     ]
    }
   ],
   "source": [
    "for i in b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2]\n",
      " [3 3 3]\n",
      " [5 6 7]\n",
      " [7 8 8]]\n"
     ]
    }
   ],
   "source": [
    "print(b[1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 5],\n",
       "       [5, 6, 7],\n",
       "       [7, 8, 8]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=b[0, :, :]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerrykim/anaconda3/envs/cldc/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cldc]",
   "language": "python",
   "name": "conda-env-cldc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
